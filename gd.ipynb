import numpy as np
import matplotlib.pyplot as plt
import time  # For timing execution

# Step 1: Generate synthetic data
# Linear relation: y = m1 * x + m2 + noise
x = np.linspace(-5, 5, 50)  # 50 points between -5 and 5
true_m1 = 3  # True slope
m2 = 2  # Constant intercept
y = true_m1 * x + m2 + np.random.normal(0, 1, size=len(x))  # Add noise

# Step 2: Define Mean Squared Error (MSE) Loss Function
def mse_loss(m1, x, y):
    """
    Computes the Mean Squared Error for a given m1.
    :param m1: Slope parameter
    :param x: Feature values
    :param y: Target values
    :return: Mean Squared Error
    """
    return np.mean((m1 * x + m2 - y) ** 2)

# Step 3: Compute loss for a range of m1 values for visualization
m1_values = np.linspace(-15, 15, 100)  # Range of m1 values
loss_values = [mse_loss(m, x, y) for m in m1_values]  # Compute losses

# Step 4: Implement  Gradient Descent
m1_gd = np.random.uniform(-15, 15)  # Start from the left side of U-curve
learning_rate = 0.01 # Step size

# Stopping condition
previous_loss = float('inf')
tolerance = 1e-4
gd_steps = []
start_time_gd = time.time()  # Start timer

while True:
    gradient = np.mean(2 * x * (m1_gd * x + m2 - y))  # Compute gradient
    m1_new = m1_gd - learning_rate * gradient  # Update step

    # Compute loss and check stopping condition
    current_loss = mse_loss(m1_new, x, y)
    if abs(previous_loss - current_loss) < tolerance:
        break

    gd_steps.append((m1_gd, mse_loss(m1_gd, x, y)))  # Log step
    m1_gd = m1_new  # Update m1
    previous_loss = current_loss  # Update loss tracker

gd_time = time.time() - start_time_gd  # Stop timer

# Step 5: Implement Linear Search (Brute Force) for Comparison
start_time_ls = time.time()  # Start timer
m1_best = min(m1_values, key=lambda m: mse_loss(m, x, y))  # Find best m1
ls_time = time.time() - start_time_ls  # Stop timer

# Step 6: Visualization
plt.figure(figsize=(10, 6))
plt.plot(m1_values, loss_values, 'g-', label="MSE vs m1 (Loss Curve)")
plt.scatter(*zip(*gd_steps), color='blue', s=20, label="Gradient Descent Steps")
plt.scatter(gd_steps[-1][0], gd_steps[-1][1], color='red', s=100, label=f"Final m1 (GD): {gd_steps[-1][0]:.3f}")
plt.axvline(m1_best, color='purple', linestyle='dashed', label=f"Best m1 (Linear Search): {m1_best:.3f}")
plt.xlabel("m1 (Slope)")
plt.ylabel("Loss (MSE)")
plt.legend()
plt.title("Gradient Descent and Linear Search on Loss Function")
plt.show()

# Step 7: Print Results
print("\nComparison of Methods:")
print(f"Gradient Descent Optimized m1: {gd_steps[-1][0]:.5f} (Time: {gd_time:.6f} sec)")
print(f"Linear Search Best m1: {m1_best:.5f} (Time: {ls_time:.6f} sec)")
